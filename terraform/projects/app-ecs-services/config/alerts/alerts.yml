groups:
- name: RE_Observe
  rules:
  - alert: RE_Observe_AlertManager_Below_Threshold
    expr: up{job="alertmanager"} == 0 and on(job) sum by(job) (up{job="alertmanager"}) <= 1
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "Service is below the expected instance Threshold"
        description: "The service name is {{ $labels.job }}. The URL experiencing the issue is {{ $labels.instance }}."
        runbook: "https://re-team-manual.cloudapps.digital/observe-support.html#re-observe-alertmanager-below-threshold"
  - alert: RE_Observe_Prometheus_Below_Threshold
    expr: up{job="prometheus"} == 0 and on(job) sum by(job) (up{job="prometheus"}) <= 1
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "Service is below the expected instance Threshold"
        description: "The service name is {{ $labels.job }}. The URL experiencing the issue is {{ $labels.instance }}."
        runbook: "https://re-team-manual.cloudapps.digital/observe-support.html#re-observe-prometheus-below-threshold"
  - alert: RE_Observe_No_FileSd_Targets
    # this expression is a little weird - count() has no value instead of 0 if there are no
    # matching metrics.  But no value isn't less than 1 so we aren't able to trigger an alert
    # that a regular count() is zero.  So we force missing values to be equal to
    # zero using the `or count(up)*0` expression.
    # This idea taken from: https://www.robustperception.io/existential-issues-with-metrics/
    #
    # Note also that this alert will only fire if there are *no* file_sd targets.
    # This is useful if we only have one source of file_sd config, but might not be
    # if we have multiple ways of receiving file_sd configs and only one of them breaks.
    expr: (count(prometheus_sd_file_mtime_seconds) or count(up)*0) < 1
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "No file_sd targets detected"
        description: "No file_sd targets were detected.  Is there a problem accessing the targets bucket?"
        runbook: "https://re-team-manual.cloudapps.digital/observe-support.html#re-observe-no-filesd-targets"

# this can be improved however maybe this is something we need to focus on in Q2 when working on the support plan

  - alert: RE_Observe_Prometheus_Over_Capacity
    expr: sum by(instance)(rate(prometheus_engine_query_duration_seconds_sum{job="prometheus"}[5m])) > 8
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "Service is over capacity."
        description: "The service name is {{ $labels.job }}. The URL experiencing the issue is {{ $labels.instance }}."
        runbook: "https://re-team-manual.cloudapps.digital/observe-support.html#re-observe-prometheus-over-capacity"

  - alert: RE_Observe_Prometheus_High_Load
    expr: sum by(instance)(rate(prometheus_engine_query_duration_seconds_sum{job="prometheus"}[2h])) > 4
    labels:
        product: "prometheus"
        severity: "ticket"
    annotations:
        summary: "Service is approaching capacity."
        description: "The service name is {{ $labels.job }}. The URL experiencing the issue is {{ $labels.instance }}."
        runbook: "https://re-team-manual.cloudapps.digital/observe-support.html#re-observe-prometheus-high-load"

  - alert: RE_Observe_Target_Down
    expr: up{} == 0
    for: 20m
    labels:
        product: "prometheus"
        severity: "ticket"
    annotations:
        summary: "{{ $labels.job }} target is down"
        description: "One of the {{ $labels.job }} targets has been down for 20 minutes"
        runbook: "https://re-team-manual.cloudapps.digital/observe-support.html#re-observe-target-down"
